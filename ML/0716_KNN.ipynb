{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN)\n",
    "\n",
    "지도학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지도 학습 (Supervised Learning)\n",
    "\n",
    "- 데이터에 대한 Label(명시적인 답)이 주어진 상태에서 컴퓨터를 학습시키는 방법\n",
    "\n",
    "## 비지도 학습 (Unsupervised Learning)\n",
    "\n",
    "- 데이터에 대한 Label(명시적인 답)이 없는 상태에서 컴퓨터를 학습시키는 방법\n",
    "\n",
    "- 데이터의 숨겨진 특성이나 구조를 파악하는데 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류 (Classification)\n",
    "\n",
    "- 미리 정의된 여러 클래스 레이블 중 하나를 예측하는 것\n",
    "\n",
    "- 속성 값을 입력, 클래스 값을 출력으로 하는 모델\n",
    "\n",
    "- 붓꽃(iris)의 세 품종 중 하나로 분류, 암 분류 등\n",
    "\n",
    "- 이진 분류, 다중 분류 등이 있다.\n",
    "\n",
    "\n",
    "### 회귀 (Regression)\n",
    "\n",
    "- 연속적인 숫자를 예측하는 것\n",
    "\n",
    "- 속성값을 입력, 연속적인 실수 값을 출력으로 하는 모델\n",
    "\n",
    "- 어떤 사람의 교육수준, 나이, 주거지를 바탕으로 연간 소득 예측\n",
    "\n",
    "- 예측 값의 미묘한 차이가 크게 중요하지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일반화, 과대적합, 과소적합\n",
    "\n",
    "### 일반화 (Generalization)\n",
    "\n",
    "- 모델이 처음보는 데이터에 대해 정확하게 예측할 수 있는 것\n",
    "\n",
    "- 훈련 세트로 학습한 모델이 테스트 세트에 대해 정확히 예측 하도록 하는 것\n",
    "\n",
    "### 과대적합 (Overfitting)\n",
    "\n",
    "- 훈련 세트에 너무 맞추어져 있어 테스트 세트의 성능 저하\n",
    "\n",
    "### 과소적합 (Underfitting)\n",
    "\n",
    "- 훈련 세트를 충분히 반영하지 못해 훈련 세트, 테스트 세트에서 모두 성능이 저하"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**일반화 성능이 최대화 되는 모델을 찾는 것이 목표**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과대적합 (Overfitting)\n",
    "\n",
    "- 모델이 훈련 세트의 각 샘플에 너무 가깝게 맞춰져서 새로운 데이터에 대해 일반화되기 어려울 때\n",
    "\n",
    "- 가진 정보를 모두 사용해서 너무 복잡한 모델을 만드는 것\n",
    "\n",
    "### 과소적합 (Underfitting)\n",
    "\n",
    "- 모델링을 너무 간단하게 하여 성능이 제대로 나오지 않을 때,\n",
    "\n",
    "- 데이터의 다양성을 잡아내지 못하고, 훈련 세트에도 잘 맞지 않는 너무 간단한 모델을 만드는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해결 방법\n",
    "\n",
    "- 주어진 훈련데이터의 다양성이 보장되어야 한다(다양한 데이터 포인트를 골고루 나타내야 한다)\n",
    "\n",
    "- 일반적으로 데이터 양이 많으면 일반화에 도움이 된다.\n",
    "\n",
    "- 그러나 편중된 데이터를 많이 모으는 것은 도움이 되지 않는다.\n",
    "\n",
    "- 규제(Regularization) 을 통해 모델의 복잡도를 정정선으로 설정한다.\n",
    "\n",
    "- [머신러닝 용어집 규제](https://developers.google.com/machine-learning/glossary#L1_regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-최근접 이웃 알고리즘\n",
    "\n",
    "- 새로운 데이터 포인트와 가장 가까운 훈련 데이터세트의 데이터 포인트를 찾아 예측\n",
    "\n",
    "- k 값에 따라 가까운 이웃의 수가 결정\n",
    "\n",
    "- 분류와 회귀에 모두 사용 가능\n",
    "\n",
    "<br/>\n",
    "\n",
    "- 입력값과 k개의 가까운 점이 있다고 가정할 때 그 점들이 어떤 라벨과 가장 비슷한지(최근접 이웃) 판단하는 알고리즘\n",
    "\n",
    "- 매개 변수: 데이터 포인트 사이의 거리를 재는 방법(일반적으로 `유클리디안 거리`를 이용), 이웃의 수\n",
    "\n",
    "- 장점: 이해하기 쉬운 모델, 약간의 조정으로 좋은 성능\n",
    "\n",
    "- 단점: 훈련 세트가 크면 속도가 느림, 많은 특성을 처리하기 힘듬\n",
    "\n",
    "<br/>\n",
    "\n",
    "- k값이 작을 수록 모델의 복잡도가 상대적으로 증가(이웃을 적게 사용하면)\n",
    "\n",
    "- 반대로 k값이 커질수록 모델의 복잡도가 낮아진다(이웃을 많이 사용하면)\n",
    "\n",
    "- 100개의 데이터를 학습하고, k를 100개로 설정하여 예측하면 빈도가 가장 많은 클래스 레이블로 분류됨(훈련 데이터 전체 개수를 이웃의 수로 지정하는 극단적인 경우)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유클리디안 거리(Euclidean distance)\n",
    "\n",
    "두 점 사이의 거리를 계산할 때 사용하는 방법\n",
    "\n",
    "- 두 점 (p1, p2, ...)와 (q1, q2, ...)의 거리\n",
    "\n",
    "<br/>\n",
    "\n",
    "유클리디안 거리 공식\n",
    "<center>\n",
    " <img src=\"https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F242971445236215901\" alt=\"유클리디안 거리 공식\" width=\"60%\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier()\n",
    "```\n",
    "KNeighborsClassifier(n_neighbors, weights, algorithm, leaf_size, p, metric, metric_params, n_jobs)\n",
    "```\n",
    "- n_neighbors : 이웃의 수 (default : 5)\n",
    "\n",
    "- weights : 예측에 사용된 가중 함수 (uniform, distance) (default : uniform)\n",
    "\n",
    "- algorithm : 가까운 이웃을 계산하는데 사용되는 알고리즘 (auto, ball_tree, kd_tree, brute)\n",
    "\n",
    "- leaf_size : BallTree 또는 KDTree에 전달 된 리프 크기\n",
    "\n",
    "- p : (1 : minkowski_distance, 2: manhattan_distance 및 euclidean_distance)\n",
    "\n",
    "- metric : 트리에 사용하는 거리 메트릭스\n",
    "\n",
    "- metric_params : 메트릭 함수에 대한 추가 키워드 인수\n",
    "\n",
    "- n_jobs : 이웃 검색을 위해 실행할 병렬 작업 수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN의 장단점\n",
    "\n",
    "- 이해하기 매우 쉬운 모델\n",
    "\n",
    "- 훈련 데이터 세트가 크면(특성, 샘플의 수) 예측이 느려진다.\n",
    "\n",
    "- 수백 개 이상의 많은 특성을 가진 데이터 세트와 특성 값 대부분이 0인 희소(sparse)한 데이터 세트에는 잘 동작하지 않는다.\n",
    "\n",
    "- 거리를 측정하기 때문에 같은 scale은 같도록 정규화 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594953019527",
   "display_name": "Python 3.7.7 64-bit ('study': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}