# PART 1. 단층 퍼셉트론(SLP)

**목차**

[TOC]

<br/>

# 1장 회귀분석

<br/>

### 이 장에서 다룰 내용

1. 단층 퍼셉트론 신경망 구조
2. 텐서 연산, 미니배치, 하이퍼파라미터, One-hot 벡터 표현
3. 신경망의 세 가지 기본 출력 유형
4. 회귀 분석, 평균제곱오차(MSE) 손실함수
5. 경사하강법, 역전파, 편미분, 손실 기울기
6. 전복 고리 수 추정 신경망 구현과 실행



<br/>

## 1.1 단층 퍼셉트론 신경망 구조

<br/>

### 단층 퍼셉트론 ?

- 일련의 퍼셉트론을 한 줄로 배치
- 입력 벡터 하나로 부터 출력 벡터 하나를 얻음
- 입력 벡터만 공유할 뿐, 각자의 파라미터에 따라 독립적 활동
- 비록 간단한 구조이지만 여러 고급 신경망 구조의 기본요소

<br/>

가중치(w)

편향값(b)



<br/>

### 파라미터(parameter)란 ?

- 학습 과정 중에서 끊임없이 변경되어 가면서 퍼셉트론의 동작 특성을 결정하는 값들



<br/>

- 모델 파라미터

- 파리미터와 타입 파라미터는 다른것이다.



<br/>

## 1.2 텐서 연산과 미니배치의 활용



<br/>

a. 퍼셉트론 하나의 동작

b. 퍼셉트론 열의 동작

c. 미니배치 데이터 처리



<br/>

### 하이퍼파라미터(hyper parameter) ?

- 에폭 수나 미니배치 크기처럼 학습과정에서 변경되지 않으면서 신경망 구조나 학습 결과에 영향을 미치는 요인



<br/>

### 에폭(epoch) ?

- 학습데이터 전체에 대한 한 차례 처리를 '에폭'이라고 부른다.



<br/>

### 미니배치(minibatch)란 ?

- 딥러닝에서는 신경망이 하나의 데이터를 여러 데이터로 나눠 한꺼번에 처리하는데 이를 '미니배치'라고 한다.



<br/>

- 입력 성분의 일차식으로 표현되는 과정을 선형 연산, 일차식으로 나타낼 수 없는 연산을 비선형 연산(활성화 함수)



<br/>

## 1.3 신경망의 세 가지 기본 출력 유형과 회귀분석

<br/>

a. 회귀 분석

b. 이진 판단

c. 선택 분류



<br/>

### 회귀(regression) ?

- 통계학에서는 연속형 변수 사이의 모형을 구한뒤 적합도를 측정하는 분석방법을 의미



<br/>

- 예시
  - 부모와 자녀의 키의 상관관계

- 입력데이터를 근거로 출력을 예측



<br/>

## 1.4 전복의 고리 수 추정 문제

- 전복의 나이를 추정
- 전복이 오래 살 수 있다 = 전복이 살기에 좋은 환경이다

<br/>

- 전복이 있는 위치에 대한 환경 정보를 파악할 수 있다.

<br/>

- 전복의 나이를 추정함으로 인해 다양한 정보를 유추 혹은 검증할 수 있다.



<br/>

## 1.5 회귀분석과  평균제곱오차(MSE) 손실 함수



<br/>

### 평균제곱오차(mean squared error) ?

- 출려 각 성분에 대한 추정값과 정답 사이의 차이인 오차를 제곱한 뒤 모두 합해 전체 성분 수로 나눈값
- 실제 정답과 비슷해지면 0에 가까워 진다.



<br/>

### 손실함수(loss function) ?

- 딥러닝에서는 평균제곱오차값이 항상 0 이상이며 추정이 정확해질 수록 값이 작아지는 성질이 있으면서 미분도 가능한 평가지표를 정의
- 이를 최소화하기위한 것이 목표
- 이러한 성질을 손실함수라고 한다.

- 비용함수



<br/>

- 함수
  - 입력에 따라 그 값이 달라지기 때문에 함수라고 부른다.



<br/>

### 한중일 매달 예측 수 (에이림 vs ROBOT_A)

- 둘의 예측에 대해 평균제곱오차를 이용해서 누가 더 예측을 잘 했는지 평가할 수 있다.
- '에이림'이 평균제곱오차의 값이 0에 더 가까우므로 더 잘 예측했다.



<br/>

## 1.6 경사하강법과 역전파



<br/>

### 경사하강법(gradient descent algorithm) ?

- 기울기에 따라 함수값이 낮아지는 방향으로 이동하는 기법



<br/>

### 순전파(forward propagation) ?

- 입력 데이터에 대해 신경망 구조를 따라가면서 현재의 파라미터값들을 이용해 손실함수값을 계산하는 과정



<br/>

### 역전파(backward propagation) ?

- 순전파의 계산과정을 역순으로 거슬러 가면서 손실 함수값에 직간접적으로 영향을 미친 모든 성분에 대해 손실 기울기를 계산하는 과정



<br/>

- 성분 x의 손실 기울기

  - 
    $$
    \frac{\partial L} {\partial x}
    $$

- 학습

  - 손실기울기에 학습률을 곱한 값을 빼는 방법

- 2차원 그래프로 표현한 경사하강법의 원리



<br/>

### 학습률(learning rate) ?

- 학습률은 임의의 양수값을 사용할 수 있지만 값이 클 수록 목표 근처에서 정확하게 바닥을 찾는 능력이 무뎌지고, 값이 작을 수록 바닥 지점에 접근하는 시간이 더 오래걸림



<br/>

### 편미분(partial derivative) ?

- x를 제외한 다른 변수 모두를 상수로 간주하고 미분하는 간소화된 미분법



<br/>

- 경사하강법은 global minimum에는 도달하기 어렵다.
- 경사하강법을 보조하는 기법들이 여러가지 있다.



<br/>

## 1.7 편미분과 손실 기울기의 계산



<br/>

### 편미분의 연쇄적 관계(chain rule) ?

- 미분의 기본성질

- $$
  \frac {\partial L} {\partial x} = \frac {\partial L} {\partial } \frac {\partial y} {\partial x}
  $$

  

<br/>

- L : 손실함수
- D : 손실함수에 대한 편미분



<br/>

### 신경망의 목적

- 손실함수가 최솟값(오차가 최소)일 때의 파라미터를 찾아 올바른 학습결과를 내는 것



<br/>

- 학습률은 너무 지나치게 작거나 너무 지나치게 크면 않된다.
- 학습률이 지나치게 작은 경우
  - 보폭이 너무 작다보니 바닥에 가는데 너무 오랜 시간이 걸린다.
- 학습률이 지나치게 큰 경우
  - 보폭이 너무 크다보니 바닥에 닿기 보단, 점점 멀어진다.(발산)



<br/>

- Adam
  - 고정된 학습률의 단점을 보완하기 위하여 학습 초반에는 큰 학습률을 사용하고, 바닥점에 가까워 질수록 학습률을 줄이는 기법



<br/>

- 옵티마이저
  - 경사하강법을 조금 더 효율적으로 만들기위해 적용하는 것





<br/>

## 1.8 하이퍼파라미터



<br/>

### 딥러닝 모델에 등장하는 네 가지 값들

1.  외부에서 주어지는 값

   - 직접 손댈 수 없는 고정된 데이터
   - 재료 투입구

   <br/>

2. 각종 중간 계산 결과

   - 생산 결과물

   <br/>

3. 파라미터

   - 가변 제어 장치
   - 파라미터의 가중치나 편향

   <br/>

4. 하이퍼파라미터

   - 고정 장치
   - 학습률, 학습횟수, 미니배치 크기



<br/>

## 1.9 비선형 정보와 원-핫 벡터 표현

<br/>

- 전복의 sex 정보
  - 유충, 수컷, 암컷
  - 3 가지의 종류로 나눠서 저장



<br/>

- 전복 성별 정보의 원-핫 벡터 표현

| 유충       | 수컷       | 암컷       |
| ---------- | ---------- | ---------- |
| infant = 1 | infant = 0 | infant = 0 |
| male = 0   | male = 1   | male = 0   |
| female = 0 | female = 0 | female = 1 |



<br/>

- 지금까지의 설명 요약
  - 정복 고리 수 추정 문제
    - 입력 벡터 크기: 10 (3 + 7)
    - 출력 벡터 크기: 1
    - 미니배치 크기: mb_size
    - 퍼셉트론의 수: 1
  
  <br/>
  
  - 단층 퍼셉트론
    - 입력: [mb_size, 10]
    - 출력 벡터 크기: [mb_size, 1]
    - 미니배치 크기: mb_size
    - 가중치 정보: [10, 1]
    - 편향 정보: [1]



<br/>

## 1.10 구현하기: 전복 고리 수 추정 신경망



<br/>

전복의 고리 수를 추정하는 프로그램을 파이썬으로 구축해보기(코드 블록 단위로 진행)



<br/>

**코드 블록**

- 구현 코드: 신경망 프로그램, 데이터셋 프로그램
- 실행 코드: 구현된 코드로 실행 및 실험 결과 출력









<br/><br/>



